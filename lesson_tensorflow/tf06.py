# -*- coding: utf-8 -*-

__author__ = "cs-pc/2018-03-26"

'''参考：
https://blog.csdn.net/jerr__y/article/details/61195257
TensorFlow入门（六） 双端 LSTM 实现序列标注（分词）

前言 
本例子主要介绍如何使用 TensorFlow 来一步一步构建双端 LSTM 网络
（听名字就感觉好腻害的样子），并完成序列标注的问题。
先声明一下，本文中采用的方法主要参考了
【中文分词系列】 4. 基于双向LSTM的seq2seq字标注这篇文章。
该文章用 keras 框架来实现的双端 LSTM，在本例中，实现思路和该文章基本上一样，
但是用 TensorFlow 来实现的。
这个例子中涉及到的知识点比较多，包括 word embedding, Viterbi 算法等，
但是就算你对这些不是非常了解，依然能够很好地理解本文。

本例的主要目的是讲清楚基于 TensorFlow 如何来实现双端 LSTM。
通过本例的学习，你可以知道 Bi-directional LSTM 是怎么样一步一步计算的。
为了讲清楚这个，我把封装好的 static_bidirectional_rnn 接口进行展开，
自己手写实现了一遍。如果你只是想急着用一下看看效果的话，
我也提供了static_bidirectional_rnn 接口的用法（其实网上多了去）。
但是既然用这个东西，当然还是希望把细节也理解透更好。
否则的话，还不如直接用 keras 几行就把模型建好了，
中间的变量维度也不需要你过多地考虑，keras 框架已经写好了自动匹配的功能
。但是你用 keras 跑了几个网络以后，一问你细节，你啥也不知道。
所以，抱着学习的心态，从 TensorFlow 这一比较底层的框架上手还是能有不少收获的。
另外，因为比较底层，我们可以比较灵活的进行模型修改（假设已经到了要改模型这一步…）

由于这个例子的代码比较长，本文主要就网络结构部分进行分析。
其余的比如数据处理这些在这里只是简单介绍，
想理解具体内容的欢迎移步 鄙人 GitHub，代码，数据 什么的全都放上去了。

如果你还不知道什么是 LSTM 的话，建议先看一下
（译）理解 LSTM 网络 （Understanding LSTM Networks by colah） 这篇文章。
在理解 LSTM 的基础上，再去理解 双端 LSTM （Bi-directional LSTM）还是非常容易的。
关于双端 LSTM 的原理，这里不做详细解释
'''





'''==========================================================================================
'''


'''==========================================================================================
'''



'''==========================================================================================
'''









'''==========================================================================================
'''


'''==========================================================================================
'''



'''==========================================================================================
'''

print("-------------------")
print("--------------------------------------")

